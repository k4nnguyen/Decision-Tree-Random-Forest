import time
import pandas as pd
import os
import re
import unicodedata
import ftfy
import nltk
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from webdriver_manager.chrome import ChromeDriverManager
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from config.keywords import tichCuc,tieuCuc
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()
analyzer = SentimentIntensityAnalyzer()

def chuanHoa(text):
    text = text.replace("ƒë", "d").replace("ƒê", "D")
    text = unicodedata.normalize('NFKD', text)
    text = text.encode('ASCII', 'ignore').decode('utf-8')
    text = re.sub(r'[^\w\s-]', '', text)
    return re.sub(r'\s+', '_', text).strip()

def chuanHoaEncoding(text):
    for encoding in ['latin1', 'cp1252']:
        try:
            text = text.encode(encoding).decode('utf-8')
            break
        except:
            continue
    text = ftfy.fix_text(text)
    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('ascii')
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def chuanHoaReview(raw_text):
    text = chuanHoaEncoding(raw_text.strip()).lower()
    text = re.sub(r'[^a-z\s]', '', text)  # lo·∫°i b·ªè s·ªë v√† k√Ω t·ª± ƒë·∫∑c bi·ªát
    tokens = nltk.word_tokenize(text)
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]
    return ' '.join(tokens)

def labelWord(text):
    scores = analyzer.polarity_scores(text)
    compound = scores['compound']
    if compound >=  0.05:
        return 1
    elif compound <= -0.05:
        return 0
    # fallback lexicon n·∫øu VADER trung t√≠nh
    text_lower = text.lower()
    pos_count = sum(1 for w in tokens if w in tichCuc)
    neg_count = sum(1 for w in tokens if w in tieuCuc)

    if pos_count > neg_count:
        return 1
    elif neg_count > pos_count:
        return 0
    else:
        return -1

def crawl_reviews(search_query):
    output_folder = "../data/raw_data"
    os.makedirs(output_folder, exist_ok=True)

    options = Options()
    options.add_argument("--start-maximized")
    service = Service(executable_path="G:/Selenium/chromedriver-win64/chromedriver.exe")  # Thay ƒë·ªïi ƒë∆∞·ªùng d·∫´n ƒë·∫øn chromedriver c·ªßa b·∫°n
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options) # Lu√¥n t∆∞∆°ng th√≠ch v·ªõi Chronium
    search_url = f"https://www.google.com/maps/search/{search_query.replace(' ', '+')}"
    driver.get(search_url)

    try:
        review_tab = WebDriverWait(driver, 10).until(
            EC.element_to_be_clickable((By.XPATH, '//button[@role="tab" and contains(@aria-label, "Reviews")]'))
        )
        review_tab.click()
        print("ƒê√£ click v√†o tab Reviews.")
    except:
        print("Kh√¥ng t√¨m th·∫•y tab Reviews.")

    try:
        scrollable_div = WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "div.m6QErb.DxyBCb.kA9KIf.dS8AEf.XiKgde"))
        )
    except:
        print("Kh√¥ng t√¨m th·∫•y khung cu·ªôn review ch√≠nh.")
        driver.quit()
        exit()

    last_height = driver.execute_script("return arguments[0].scrollHeight", scrollable_div)
    start_time = time.time()
    max_wait = 60

    for i in range(100):
        driver.execute_script("arguments[0].scrollTop = arguments[0].scrollHeight", scrollable_div)
        time.sleep(1.5)
        new_height = driver.execute_script("return arguments[0].scrollHeight", scrollable_div)
        print(f"==> Cu·ªôn v√≤ng {i+1}, chi·ªÅu cao: {new_height}")

        if new_height == last_height:
            print("Kh√¥ng c√≤n review m·ªõi ƒë·ªÉ t·∫£i.")
            break

        if time.time() - start_time > max_wait:
            print("H·∫øt th·ªùi gian cu·ªôn.")
            break

        last_height = new_height

    print("‚úÖ ƒê√£ cu·ªôn xong v√† s·∫µn s√†ng thu th·∫≠p review.")

    review_blocks = driver.find_elements(By.CSS_SELECTOR, 'div.jJc9Ad')
    data = []

    for block in review_blocks:
        try:
            if block.find_elements(By.CSS_SELECTOR, 'div.GvZfFd > div.Jtu6Td'):
                continue
            review_text_elem = block.find_element(By.CLASS_NAME, 'wiI7pd')
            raw_text = review_text_elem.text.strip()
            processed_text = chuanHoaReview(raw_text)
            label = labelWord(processed_text)
            if label != -1:
                data.append({"Review": processed_text, "Label": label})
        except Exception:
            continue

    driver.quit()

    filename = chuanHoa(search_query) + ".csv"
    filepath = os.path.join(output_folder, filename)
    df = pd.DataFrame(data)
    df.to_csv(filepath, index=False, encoding="utf-8")
    print(f"üíæ ƒê√£ l∆∞u {len(df)} review c√≥ nh√£n v√†o file '{filepath}' th√†nh c√¥ng!")